{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fb2bec4",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## INSTALLING AND IMPORTING NLTK LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d22d2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: NLTK in c:\\users\\dell\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\anaconda3\\lib\\site-packages (from NLTK) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\anaconda3\\lib\\site-packages (from NLTK) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from NLTK) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\anaconda3\\lib\\site-packages (from NLTK) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from click->NLTK) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54600aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c65b5074",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "feec91ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54900470",
   "metadata": {},
   "source": [
    "## Word tokenize and sentence tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87a9f7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd288d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "para='Michael Shawn Hickenbottom, better known by his ring name Shawn Michaels, is an American retired professional wrestler.He is signed to WWE, where he is the Senior Vice President of Talent Development Creative and oversees the creative aspects of the NXT brand, the promotion developmental territory.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46ba621f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael',\n",
       " 'Shawn',\n",
       " 'Hickenbottom',\n",
       " ',',\n",
       " 'better',\n",
       " 'known',\n",
       " 'by',\n",
       " 'his',\n",
       " 'ring',\n",
       " 'name',\n",
       " 'Shawn',\n",
       " 'Michaels',\n",
       " ',',\n",
       " 'is',\n",
       " 'an',\n",
       " 'American',\n",
       " 'retired',\n",
       " 'professional',\n",
       " 'wrestler.He',\n",
       " 'is',\n",
       " 'signed',\n",
       " 'to',\n",
       " 'WWE',\n",
       " ',',\n",
       " 'where',\n",
       " 'he',\n",
       " 'is',\n",
       " 'the',\n",
       " 'Senior',\n",
       " 'Vice',\n",
       " 'President',\n",
       " 'of',\n",
       " 'Talent',\n",
       " 'Development',\n",
       " 'Creative',\n",
       " 'and',\n",
       " 'oversees',\n",
       " 'the',\n",
       " 'creative',\n",
       " 'aspects',\n",
       " 'of',\n",
       " 'the',\n",
       " 'NXT',\n",
       " 'brand',\n",
       " ',',\n",
       " 'the',\n",
       " 'promotion',\n",
       " 'developmental',\n",
       " 'territory',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c9c7c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael',\n",
       " 'Shawn',\n",
       " 'Hickenbottom,',\n",
       " 'better',\n",
       " 'known',\n",
       " 'by',\n",
       " 'his',\n",
       " 'ring',\n",
       " 'name',\n",
       " 'Shawn',\n",
       " 'Michaels,',\n",
       " 'is',\n",
       " 'an',\n",
       " 'American',\n",
       " 'retired',\n",
       " 'professional',\n",
       " 'wrestler.He',\n",
       " 'is',\n",
       " 'signed',\n",
       " 'to',\n",
       " 'WWE,',\n",
       " 'where',\n",
       " 'he',\n",
       " 'is',\n",
       " 'the',\n",
       " 'Senior',\n",
       " 'Vice',\n",
       " 'President',\n",
       " 'of',\n",
       " 'Talent',\n",
       " 'Development',\n",
       " 'Creative',\n",
       " 'and',\n",
       " 'oversees',\n",
       " 'the',\n",
       " 'creative',\n",
       " 'aspects',\n",
       " 'of',\n",
       " 'the',\n",
       " 'NXT',\n",
       " 'brand,',\n",
       " 'the',\n",
       " 'promotion',\n",
       " 'developmental',\n",
       " 'territory.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "614dbb80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael Shawn Hickenbottom, better known by his ring name Shawn Michaels, is an American retired professional wrestler',\n",
       " 'He is signed to WWE, where he is the Senior Vice President of Talent Development Creative and oversees the creative aspects of the NXT brand, the promotion developmental territory',\n",
       " '']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0da0d9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael Shawn Hickenbottom, better known by his ring name Shawn Michaels, is an American retired professional wrestler.He is signed to WWE, where he is the Senior Vice President of Talent Development Creative and oversees the creative aspects of the NXT brand, the promotion developmental territory.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e02d425",
   "metadata": {},
   "source": [
    "## Stopwords and stopwords Customization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "645573bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39abdee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b72770fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Michael Shawn Hickenbottom, better known by his ring name Shawn Michaels, is an American retired professional wrestler.He is signed to WWE, where he is the Senior Vice President of Talent Development Creative and oversees the creative aspects of the NXT brand, the promotion developmental territory.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaa67f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael',\n",
       " 'Shawn',\n",
       " 'Hickenbottom',\n",
       " ',',\n",
       " 'better',\n",
       " 'known',\n",
       " 'by',\n",
       " 'his',\n",
       " 'ring',\n",
       " 'name',\n",
       " 'Shawn',\n",
       " 'Michaels',\n",
       " ',',\n",
       " 'is',\n",
       " 'an',\n",
       " 'American',\n",
       " 'retired',\n",
       " 'professional',\n",
       " 'wrestler.He',\n",
       " 'is',\n",
       " 'signed',\n",
       " 'to',\n",
       " 'WWE',\n",
       " ',',\n",
       " 'where',\n",
       " 'he',\n",
       " 'is',\n",
       " 'the',\n",
       " 'Senior',\n",
       " 'Vice',\n",
       " 'President',\n",
       " 'of',\n",
       " 'Talent',\n",
       " 'Development',\n",
       " 'Creative',\n",
       " 'and',\n",
       " 'oversees',\n",
       " 'the',\n",
       " 'creative',\n",
       " 'aspects',\n",
       " 'of',\n",
       " 'the',\n",
       " 'NXT',\n",
       " 'brand',\n",
       " ',',\n",
       " 'the',\n",
       " 'promotion',\n",
       " 'developmental',\n",
       " 'territory',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ff8c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "for i in word_tokenize(para):\n",
    "    if i in stop:\n",
    "        continue\n",
    "    else:\n",
    "        data.append(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6dc8813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael',\n",
       " 'Shawn',\n",
       " 'Hickenbottom',\n",
       " ',',\n",
       " 'better',\n",
       " 'known',\n",
       " 'ring',\n",
       " 'name',\n",
       " 'Shawn',\n",
       " 'Michaels',\n",
       " ',',\n",
       " 'American',\n",
       " 'retired',\n",
       " 'professional',\n",
       " 'wrestler.He',\n",
       " 'signed',\n",
       " 'WWE',\n",
       " ',',\n",
       " 'Senior',\n",
       " 'Vice',\n",
       " 'President',\n",
       " 'Talent',\n",
       " 'Development',\n",
       " 'Creative',\n",
       " 'oversees',\n",
       " 'creative',\n",
       " 'aspects',\n",
       " 'NXT',\n",
       " 'brand',\n",
       " ',',\n",
       " 'promotion',\n",
       " 'developmental',\n",
       " 'territory',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9b9bf8",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d54ce33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90ab880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "red=['plays','fruitful','playing','sunny','peacefully']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bbb7599a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plays', 'fruitful', 'playing', 'sunny', 'peacefully']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff8ea164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c600ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db28cbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fruit'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_.stem('fruitful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b11c622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plays play\n",
      "fruitful fruit\n",
      "playing play\n",
      "sunny sunni\n",
      "peacefully peac\n"
     ]
    }
   ],
   "source": [
    "for i in red:\n",
    "    print(i,stem_.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "589a3cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64aac034",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d9d9c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'peacefully'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('peacefully','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9b93e0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plays - play\n",
      "fruitful - fruitful\n",
      "playing - play\n",
      "sunny - sunny\n",
      "peacefully - peacefully\n"
     ]
    }
   ],
   "source": [
    "for i in red:\n",
    "    print(i,'-',lemma.lemmatize(i,'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd6eaa",
   "metadata": {},
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f958059",
   "metadata": {},
   "outputs": [],
   "source": [
    "document =['One Geek helps Two Geeks','Two Geeks help Four Geeks','Each Geek helps many other Geeks at GeeksforGeeks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fe4d35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One Geek helps Two Geeks',\n",
       " 'Two Geeks help Four Geeks',\n",
       " 'Each Geek helps many other Geeks at GeeksforGeeks']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b505430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcec41a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "count=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "305ffc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat=count.fit_transform(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d5f256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a='.'.join(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f286c256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One Geek helps Two Geeks.Two Geeks help Four Geeks.Each Geek helps many other Geeks at GeeksforGeeks'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc190442",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=set(word_tokenize(a.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80b13f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'at',\n",
       " 'four',\n",
       " 'geek',\n",
       " 'geeks',\n",
       " 'geeks.each',\n",
       " 'geeks.two',\n",
       " 'geeksforgeeks',\n",
       " 'help',\n",
       " 'helps',\n",
       " 'many',\n",
       " 'one',\n",
       " 'other',\n",
       " 'two'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd6be0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1=[]\n",
    "for i in cols:\n",
    "    if i in stop:\n",
    "        continue\n",
    "    else:\n",
    "        d1.append(i)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a624720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['help',\n",
       " 'geeks.each',\n",
       " 'two',\n",
       " 'geeksforgeeks',\n",
       " 'many',\n",
       " 'one',\n",
       " 'four',\n",
       " 'geeks.two',\n",
       " 'geek',\n",
       " 'helps',\n",
       " 'geeks']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc135f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d9f1b52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2acc72cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x12 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 17 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2275e86",
   "metadata": {},
   "source": [
    "pd.DataFrame(mat.toarray(),columns=list(cols),index=document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b3f825",
   "metadata": {},
   "source": [
    "## tf-idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "95c07706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6f374664",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=['this is the first doucment.','this is the second doucment.','and this is the third doucment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b18a5881",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "70fcdf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=vector.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f63fb3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'doucment' 'first' 'is' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "print(vector.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "65255a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 8)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aa5f2871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>doucment</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>this is the first doucment.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381614</td>\n",
       "      <td>0.646129</td>\n",
       "      <td>0.381614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this is the second doucment.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381614</td>\n",
       "      <td>0.646129</td>\n",
       "      <td>0.381614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and this is the third doucment</th>\n",
       "      <td>0.542701</td>\n",
       "      <td>0.320528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.320528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.320528</td>\n",
       "      <td>0.542701</td>\n",
       "      <td>0.320528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     and  doucment     first        is  \\\n",
       "this is the first doucment.     0.000000  0.381614  0.646129  0.381614   \n",
       "this is the second doucment.    0.000000  0.381614  0.000000  0.381614   \n",
       "and this is the third doucment  0.542701  0.320528  0.000000  0.320528   \n",
       "\n",
       "                                  second       the     third      this  \n",
       "this is the first doucment.     0.000000  0.381614  0.000000  0.381614  \n",
       "this is the second doucment.    0.646129  0.381614  0.000000  0.381614  \n",
       "and this is the third doucment  0.000000  0.320528  0.542701  0.320528  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x.toarray(),columns=vector.get_feature_names_out(),index=corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eb50340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5979f6",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b439e614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from  nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "676f8162",
   "metadata": {},
   "outputs": [],
   "source": [
    "port=['tommorrow iam going to kill my girlfreind','we will help all the people of our locality with pure heart','delhi is just 50km from here']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "369d625f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tommorrow iam going to kill my girlfreind',\n",
       " 'we will help all the people of our locality with pure heart',\n",
       " 'delhi is just 50km from here']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfb2f80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tommorrow', 'iam', 'going', 'to', 'kill', 'my', 'girlfreind'],\n",
       " ['we',\n",
       "  'will',\n",
       "  'help',\n",
       "  'all',\n",
       "  'the',\n",
       "  'people',\n",
       "  'of',\n",
       "  'our',\n",
       "  'locality',\n",
       "  'with',\n",
       "  'pure',\n",
       "  'heart'],\n",
       " ['delhi', 'is', 'just', '50km', 'from', 'here']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x:word_tokenize(x.lower()),port))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "139063ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "for i in port:\n",
    "    words.append(word_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73169c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tommorrow', 'iam', 'going', 'to', 'kill', 'my', 'girlfreind'],\n",
       " ['we',\n",
       "  'will',\n",
       "  'help',\n",
       "  'all',\n",
       "  'the',\n",
       "  'people',\n",
       "  'of',\n",
       "  'our',\n",
       "  'locality',\n",
       "  'with',\n",
       "  'pure',\n",
       "  'heart'],\n",
       " ['delhi', 'is', 'just', '50km', 'from', 'here']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "feaa61f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90d46b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tommorrow', 'iam', 'going', 'kill', 'girlfreind'],\n",
       " ['will', 'help', 'the', 'people', 'our', 'locality', 'pure', 'heart'],\n",
       " ['delhi', 'just', '50km', 'here']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in words:\n",
    "    for j in i:\n",
    "        if j not in stopwords.words('english'):\n",
    "            pass\n",
    "        else:\n",
    "            i.remove(j)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d9d5f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20032c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj1=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5667d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in words:\n",
    "    for j in range(len(i)):\n",
    "        i[j]=obj1.lemmatize(i[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "acd61475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tommorrow', 'iam', 'going', 'kill', 'girlfreind'],\n",
       " ['will', 'help', 'the', 'people', 'our', 'locality', 'pure', 'heart'],\n",
       " ['delhi', 'just', '50km', 'here']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19a21a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for i in words:\n",
    "    s=' '.join(i)\n",
    "    l.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b372bace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tommorrow iam going kill girlfreind',\n",
       " 'will help the people our locality pure heart',\n",
       " 'delhi just 50km here']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6a38150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc76a96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer=SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "747a261f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.polarity_scores('siddharth is good boy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a045edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tommorrow iam going kill girlfreind   {'neg': 0.54, 'neu': 0.46, 'pos': 0.0, 'compound': -0.6908}\n",
      "will help the people our locality pure heart   {'neg': 0.0, 'neu': 0.722, 'pos': 0.278, 'compound': 0.4019}\n",
      "delhi just 50km here   {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
     ]
    }
   ],
   "source": [
    "for i in l:\n",
    "    print(i,' ', analyzer.polarity_scores(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53ea191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa79e0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=['two IT workers were killed when their motorbike was struck by a speeding Porsche that was reportedly being driven by the young child.','Neeraj Chopra made history by becoming the first Indian track and field athlete to win a gold medal at the Olympic Games.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c048081c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['two IT workers were killed when their motorbike was struck by a speeding Porsche that was reportedly being driven by the young child.',\n",
       " 'Neeraj Chopra made history by becoming the first Indian track and field athlete to win a gold medal at the Olympic Games.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b84b168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['two',\n",
       "  'it',\n",
       "  'workers',\n",
       "  'were',\n",
       "  'killed',\n",
       "  'when',\n",
       "  'their',\n",
       "  'motorbike',\n",
       "  'was',\n",
       "  'struck',\n",
       "  'by',\n",
       "  'a',\n",
       "  'speeding',\n",
       "  'porsche',\n",
       "  'that',\n",
       "  'was',\n",
       "  'reportedly',\n",
       "  'being',\n",
       "  'driven',\n",
       "  'by',\n",
       "  'the',\n",
       "  'young',\n",
       "  'child',\n",
       "  '.'],\n",
       " ['neeraj',\n",
       "  'chopra',\n",
       "  'made',\n",
       "  'history',\n",
       "  'by',\n",
       "  'becoming',\n",
       "  'the',\n",
       "  'first',\n",
       "  'indian',\n",
       "  'track',\n",
       "  'and',\n",
       "  'field',\n",
       "  'athlete',\n",
       "  'to',\n",
       "  'win',\n",
       "  'a',\n",
       "  'gold',\n",
       "  'medal',\n",
       "  'at',\n",
       "  'the',\n",
       "  'olympic',\n",
       "  'games',\n",
       "  '.']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x:word_tokenize(x.lower()),sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bd3541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "for i in sent:\n",
    "    words.append(word_tokenize(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da2806c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['two',\n",
       "  'IT',\n",
       "  'workers',\n",
       "  'were',\n",
       "  'killed',\n",
       "  'when',\n",
       "  'their',\n",
       "  'motorbike',\n",
       "  'was',\n",
       "  'struck',\n",
       "  'by',\n",
       "  'a',\n",
       "  'speeding',\n",
       "  'Porsche',\n",
       "  'that',\n",
       "  'was',\n",
       "  'reportedly',\n",
       "  'being',\n",
       "  'driven',\n",
       "  'by',\n",
       "  'the',\n",
       "  'young',\n",
       "  'child',\n",
       "  '.'],\n",
       " ['Neeraj',\n",
       "  'Chopra',\n",
       "  'made',\n",
       "  'history',\n",
       "  'by',\n",
       "  'becoming',\n",
       "  'the',\n",
       "  'first',\n",
       "  'Indian',\n",
       "  'track',\n",
       "  'and',\n",
       "  'field',\n",
       "  'athlete',\n",
       "  'to',\n",
       "  'win',\n",
       "  'a',\n",
       "  'gold',\n",
       "  'medal',\n",
       "  'at',\n",
       "  'the',\n",
       "  'Olympic',\n",
       "  'Games',\n",
       "  '.']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a413209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['two',\n",
       "  'IT',\n",
       "  'workers',\n",
       "  'killed',\n",
       "  'their',\n",
       "  'motorbike',\n",
       "  'struck',\n",
       "  'a',\n",
       "  'speeding',\n",
       "  'Porsche',\n",
       "  'was',\n",
       "  'reportedly',\n",
       "  'driven',\n",
       "  'the',\n",
       "  'young',\n",
       "  'child',\n",
       "  '.'],\n",
       " ['Neeraj',\n",
       "  'Chopra',\n",
       "  'made',\n",
       "  'history',\n",
       "  'becoming',\n",
       "  'first',\n",
       "  'Indian',\n",
       "  'track',\n",
       "  'field',\n",
       "  'athlete',\n",
       "  'win',\n",
       "  'gold',\n",
       "  'medal',\n",
       "  'the',\n",
       "  'Olympic',\n",
       "  'Games',\n",
       "  '.']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in words:\n",
    "    for j in i:\n",
    "        if j not in stopwords.words('english'):\n",
    "            pass\n",
    "        else:\n",
    "            i.remove(j)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "902860dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj2=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c366246",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in words:\n",
    "    for j in range(len(i)):\n",
    "        i[j]=obj2.lemmatize(i[j])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "926960ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['two',\n",
       "  'IT',\n",
       "  'worker',\n",
       "  'killed',\n",
       "  'their',\n",
       "  'motorbike',\n",
       "  'struck',\n",
       "  'a',\n",
       "  'speeding',\n",
       "  'Porsche',\n",
       "  'wa',\n",
       "  'reportedly',\n",
       "  'driven',\n",
       "  'the',\n",
       "  'young',\n",
       "  'child',\n",
       "  '.'],\n",
       " ['Neeraj',\n",
       "  'Chopra',\n",
       "  'made',\n",
       "  'history',\n",
       "  'becoming',\n",
       "  'first',\n",
       "  'Indian',\n",
       "  'track',\n",
       "  'field',\n",
       "  'athlete',\n",
       "  'win',\n",
       "  'gold',\n",
       "  'medal',\n",
       "  'the',\n",
       "  'Olympic',\n",
       "  'Games',\n",
       "  '.']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5133cde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=[]\n",
    "for i in words:\n",
    "    t=' '.join(i)\n",
    "    s.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cd7b720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['two IT worker killed their motorbike struck a speeding Porsche wa reportedly driven the young child .',\n",
       " 'Neeraj Chopra made history becoming first Indian track field athlete win gold medal the Olympic Games .']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23a8fd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fee3e7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two IT worker killed their motorbike struck a speeding Porsche wa reportedly driven the young child .   {'neg': 0.333, 'neu': 0.667, 'pos': 0.0, 'compound': -0.7579}\n",
      "Neeraj Chopra made history becoming first Indian track field athlete win gold medal the Olympic Games .   {'neg': 0.0, 'neu': 0.67, 'pos': 0.33, 'compound': 0.7845}\n"
     ]
    }
   ],
   "source": [
    "for i in s:\n",
    "    print(i,' ',a1.polarity_scores(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395c2c69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
